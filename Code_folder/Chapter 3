---
title: "Cross-year CB"
author: "Francesco Civita"
date: "13/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries

```{r, message = F}


library(dplyr)
library(skimr)
library(tidyr)
library(lubridate)
library(janitor)
library(tidymodels)
library(tune)
library(tidyverse)
library(tictoc)

```
# Clean r's brain

```{r, message = FALSE}

rm(list=ls())

```

# Import data

```{r}
all<-readRDS("C:/Users/Avogadro/PhD Blackleg/ML/NCV/Block_year/all.rds")


Outer_mlr<-readRDS("C:/Users/Avogadro/PhD Blackleg/ML/NCV/Block_year/Outer_mlr.rds")
Inner_mlr<-readRDS("C:/Users/Avogadro/PhD Blackleg/ML/NCV/Block_year/Inner_mlr.rds")
manual_cv<-readRDS("C:/Users/Avogadro/PhD Blackleg/ML/NCV/Block_year/manual_cv.rds")
```




```{r}
ens_rec<-list()

for(j in 1:5){
ens_rec[[j]]<- recipe(bleg ~ ., data = all[Outer_mlr$train.inds[[j]],] %>% select(-year)) %>%
    update_role(crop_id, new_role = "Id") 
 
   }

   skim(juice(prep(ens_rec[[1]])))
```


```{r}
library(catboost)
library(treesnip)
set_dependency("boost_tree", eng = "catboost", "catboost")
set_dependency("boost_tree", eng = "catboost", "treesnip")

catboost_spec<-(boost_tree(
    mtry = tune(),
    trees = 150,
    tree_depth = tune(),
    learn_rate = tune(),
    stop_iter = 10)) %>% 
      set_mode("classification") %>%
      set_engine("catboost", nthread=4)

catboost_wf<-list()
for(j in 1:5){
catboost_wf[[j]]<-workflow() %>%
  add_recipe(ens_rec[[j]]) %>%
  add_model(catboost_spec)}

# get info on the hyperparameter to try
set.seed(123)
hp_arg_catboost<-tune_args(catboost_wf[[j]], verbose =T)
hp_grid_catboost<-dials::parameters(catboost_wf[[1]]) %>%
      update(learn_rate = learn_rate(range = c(-1,-2))) %>%
      update(tree_depth = tree_depth(range = c(6,10))) %>%
      update(mtry = mtry(range = c(3, 20)))%>%
  grid_max_entropy(size=30) 
  


library(tictoc)
library(doParallel)
cl<-makePSOCKcluster(4)
registerDoParallel(cl)
tic()

gs_catboost_res<-list()
#try different seeds
set.seed(123, kind = "L'Ecuyer-CMRG")
#set.seed(2020) #it seems to be irrelevant with lasso. It might depend only on seeds from initial splitting
for(j in 1:5){
  gs_catboost_res[[j]] <- tune_grid(
    catboost_wf[[j]], # Model workflow defined above
    resamples = manual_cv[[j]],          # Resamples defined obove
    grid = hp_grid_catboost,            # Number of candidate parameter sets to be created    automatically
    metrics = metric_set(accuracy,
                         pr_auc,
                         roc_auc,
                         sens,
                         yardstick::spec,
                         yardstick::precision,
                         yardstick::f_meas,
                         yardstick::mcc,
                         yardstick::mn_log_loss
    ),    # metric
    control = control_grid(save_workflow = T, save_pred = TRUE, verbose = TRUE, event_level = "second",
                           extract = function (x) extract_fit_engine(x)))}# control the tuning process



toc()
stopCluster(cl)

#3136.03
```
```{r}
unregister <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

unregister()
```
### Save tune trees

```{r}
saveRDS(gs_catboost_res, "gs_catboost_res.rds")

```

### Check catboost metrics vs penalty (lambda)

### Collect metrics catboost

```{r}


metric_catboost<-list()
for (j in 1:5) {metric_catboost[[j]]<- gs_catboost_res[[j]] %>%
  collect_metrics()}

```

```{r}
saveRDS(metric_catboost, "metric_catboost_cy.rds")
```

### Best AUROC catboost

```{r}
best_auc_gs_catboost<-list()
for(j in 1:5){
  best_auc_gs_catboost[[j]]<-select_best(gs_catboost_res[[j]], "roc_auc")}


```


### Collect metrics catboost Sf
```{r}
metric_catboost_Sf<-list()
for (j in 1:5) {metric_catboost_Sf[[j]]<- gs_catboost_res[[j]] %>%
  collect_metrics(summarize=F) %>% 
  filter(.config==best_auc_gs_catboost[[j]]$.config)%>%
           mutate(model="catboost")}

saveRDS(metric_catboost_Sf, "metric_catboost_Sf.rds")
```

### Collect predictions catboost
```{r}
predictions<-list()
for(i in 1:5) {predictions[[i]]<-collect_predictions(gs_catboost_res[[i]], summarize=F) %>% filter(.config == best_auc_gs_catboost[[i]]$.config) %>%
  select(id,.row, bleg, catboost_Yes=.pred_Yes, catboost_No = .pred_No, catboost_pred=.pred_class)}

saveRDS(predictions, "predictions_catboost.rds")

```

#### Finalize catboost (best AUROC)
```{r}

final_catboost<-list()
for(j in 1:5){
  final_catboost[[j]]<-catboost_wf[[j]] %>% 
    finalize_workflow(best_auc_gs_catboost[[j]])}
```

#### Last fit catboost (best AUROC)

```{r}

cl<-makePSOCKcluster(4)
registerDoParallel(cl)
tic()

#try different seeds
set.seed(123, kind = "L'Ecuyer-CMRG")
catboost_fit<-list()
for(j in 1:5){
  catboost_fit[[j]]<-final_catboost[[j]] %>%
    fit(all[Outer_mlr$train.inds[[j]],])}

toc()
stopCluster(cl)
unregister()
```


```{r}
model<-extract_fit_engine(catboost_fit[[1]])


model$feature_importances %>% as.data.frame() %>%arrange(desc(V1))

````

### Predict test set 

```{r}


# permance and statistics


# train set predictions
catboost_training_pred<-list()
for(j in 1:5){catboost_training_pred[[j]] <- 
  predict(catboost_fit[[j]], all[Outer_mlr$train.inds[[j]],]) %>% 
  bind_cols(predict(catboost_fit[[j]],all[Outer_mlr$train.inds[[j]],], type = "prob")) %>% 
  bind_cols(all[Outer_mlr$train.inds[[j]],] %>% select(bleg))}

catboost_training_perf<-list()
for(j in 1:5) {catboost_training_perf[[j]]<-catboost_training_pred[[j]] %>% 
  accuracy(truth = bleg, .pred_class) %>%
  bind_rows(catboost_training_pred[[j]]%>% roc_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
  bind_rows(catboost_training_pred[[j]]%>% sens(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(catboost_training_pred[[j]] %>% yardstick::spec(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(catboost_training_pred[[j]] %>% yardstick::precision(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(catboost_training_pred[[j]] %>% yardstick::f_meas(truth = bleg, .pred_class, event_level="second"))}

catboost_train_cm<-list()
for(j in 1:5) {catboost_train_cm[[j]]<-catboost_training_pred[[j]] %>% yardstick::conf_mat(truth = bleg, .pred_class)}

# catboost_train_sum<-list()
# for (j in 1:5){
# catboost_train_sum[[j]]<-summary(catboost_train_cm[[j]], event_level = "second")}


# test set predictions



# train set predictions
catboost_testing_pred<-list()
for(j in 1:5){catboost_testing_pred[[j]] <- 
  predict(catboost_fit[[j]], all[Outer_mlr$test.inds[[j]],]) %>% 
  bind_cols(predict(catboost_fit[[j]],all[Outer_mlr$test.inds[[j]],], type = "prob")) %>% 
  bind_cols(all[Outer_mlr$test.inds[[j]],] %>% select(bleg))}

catboost_testing_perf<-list()
for(j in 1:5) {catboost_testing_perf[[j]]<-catboost_testing_pred[[j]] %>% 
  accuracy(truth = bleg, .pred_class) %>%
  bind_rows(catboost_testing_pred[[j]]%>% roc_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
  bind_rows(catboost_testing_pred[[j]]%>% sens(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(catboost_testing_pred[[j]] %>% yardstick::spec(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(catboost_testing_pred[[j]] %>% yardstick::precision(truth = bleg, .pred_class, event_level="second"))%>%
   bind_rows(catboost_testing_pred[[j]] %>%yardstick::f_meas(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(
    mean((
    catboost_testing_pred[[j]]$.pred_Yes - (as.numeric(catboost_testing_pred[[j]]$bleg)-1))^2) %>% 
        as.tibble() %>% 
        bind_cols(.metric ="brier_score",.estimator="binary") %>% 
        mutate(.estimate = value, .keep="unused"))
}

catboost_test_cm<-list()
for(j in 1:5) {catboost_test_cm[[j]]<-catboost_testing_pred[[j]] %>% yardstick::conf_mat(truth = bleg, .pred_class)}

# catboost_test_sum<-list()
# for (j in 1:5){
# catboost_test_sum[[j]]<-summary(catboost_test_cm[[j]], event_level = "second")}

saveRDS( bind_rows(catboost_testing_perf, .id="Fold") %>% mutate(seed=123), "catboost_testing_perf.rds")
saveRDS( bind_rows(catboost_training_perf, .id="Fold") %>% mutate(seed=123), "catboost_training_perf.rds")

```



# aggregate validation metrics (summarize = T)
```{r}
validation<-list()
for(j in 1:5) {validation [[j]]<-metric_catboost[[j]] %>% filter(.config == best_auc_gs_catboost[[j]]$.config)%>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
validation<-do.call("rbind", validation)

```

# aggregate validation metrics (summarize = F)
```{r}
validation_sF<-list()
for(j in 1:5) {validation_sF[[j]]<-collect_metrics(gs_catboost_res[[j]],summarize=F) %>% filter(.config == best_auc_gs_catboost[[j]]$.config)%>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
validation_sF<-do.call("rbind", validation_sF) %>% 
  group_by(.metric) %>% summarise_at(.vars=".estimate", funs(mean=mean, se=sd(.)/sqrt(n()), n = n()))

```

# aggregate train metrics

```{r}
train<-list()
for(j in 1:5) {train[[j]]<-catboost_training_perf[[j]] %>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
train<-do.call("rbind", train) %>% 
  group_by(.metric) %>% summarise_at(.vars=".estimate", funs(mean=mean, se=sd(.)/sqrt(n()), n = n()))

```


# aggregate test metrics

```{r}
test<-list()
for(j in 1:5) {test[[j]]<-catboost_testing_perf[[j]] %>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
test<-do.call("rbind", test) %>% 
  group_by(.metric) %>% summarise_at(.vars=".estimate", funs(mean=mean, se=sd(.)/sqrt(n()), n = n()))

```

# compare models
```{r}
train
validation_sF
test

```


```{r}
final_score<-train %>%
    mutate(split ="train") %>%
    bind_rows(validation_sF %>% mutate(split = "validation"))%>%
    bind_rows(test %>% mutate(split = "test"))
write.csv(final_score, "final_score_catboost_By_5x5.csv")
```
#best_hp
```{r}
best_hp<-do.call("rbind", best_auc_gs_catboost) %>%
  mutate(Split=c("Split1", "Split2", "Split3", "Split4", "Split5"))
write.csv(best_hp, "best_hp.csv")
```

## Save Predictions test set

```{r}
predictions_test<-list()
for(i in 1:5) {predictions_test[[i]]<-catboost_testing_pred[[i]] %>%
  select(bleg, catboost_Yes=.pred_Yes, catboost_No = .pred_No, catboost_pred=.pred_class) %>% bind_cols(all[Outer_mlr$test.inds[[i]],] %>% select(crop_id))}

saveRDS(predictions_test, "predictions_test_catboost.rds")

```


# Export not aggregated test results

```{r}
catboost_test_results<-list()
for (i in 1:5){catboost_test_results[[i]]<-catboost_testing_perf[[i]] %>% mutate(id =paste("Fold",i),
                                                                           model = "catboost")} 
catboost_test_results<-do.call("rbind", catboost_test_results)

saveRDS(catboost_test_results, "catboost_test_results.rds")
```



#CUTOFF

```{r}
prediction<-list()

for (k in 1:5) {

prediction[[k]]<-catboost_testing_pred[[k]]%>%
  mutate(correct = case_when(
    bleg == .pred_class ~ "Correct",
    TRUE ~ "Incorrect"
  )) %>%
   bind_cols( all[Outer_mlr$test.inds[[k]],] %>% select(-bleg))}


```

`


```{r}
library(PresenceAbsence)
binary_outcome<-list()
for(j in 1:5){
binary_outcome[[j]]<-prediction[[j]]%>% dplyr::select(crop_id, bleg, .pred_Yes, .pred_No) %>% mutate(bleg = ifelse(bleg == "Yes", 1 ,0))}

```


```{r}    
#check a series of threshold use the same already defined !!
set.seed(123)
thresholds <- seq(0.001,1, by = 0.001)

optimal<-list()
optimal2<-list()

#for (i in 1:9){optimal[[i]]<-optimal.thresholds(binary_outcome, thresholds, opt.methods = i)}


for(j in 1:5){{
 optimal[[10]]<-optimal.thresholds(binary_outcome[[j]], thresholds, opt.methods = 10, req.sens = 0.75)
 optimal[[11]]<-optimal.thresholds(binary_outcome[[j]], thresholds, opt.methods = 11, req.spec = 0.97)}
  
optimal2[[j]]<-optimal}

rm(optimal)
```

```{r}

optimised_pred<-list()
optimised_pred2<-list()

# for (i in 1:9) {optimised_pred[[i]]<-binary_outcome%>%
#         mutate(hardYes = ifelse(pred_Yes>=optimal[[i]]$pred_Yes,"Yes","No")) %>%
#         mutate(bleg = ifelse(bleg4>0, "Yes", "No"))}


for(j in 1:5){{
optimised_pred[[10]]<-binary_outcome[[j]] %>%
        mutate(hardYes = ifelse(.pred_Yes>=optimal2[[j]][[10]]$.pred_Yes,"Yes","No")) %>%
        mutate(bleg = ifelse(bleg>0, "Yes", "No"))

optimised_pred[[11]]<-binary_outcome[[j]] %>%
        mutate(hardYes = ifelse(.pred_Yes>=optimal2[[j]][[11]]$.pred_Yes,"Yes","No")) %>%
        mutate(bleg = ifelse(bleg>0, "Yes", "No"))}
  
optimised_pred2[[j]]<-optimised_pred}

rm(optimised_pred)
```


```{r}

conf_mat<-list()
conf_mat2<-list()

# for (i in 1:7) {conf_mat[[i]]<-summary(optimised_pred[[i]]%>% 
#                                              mutate_if(is.character, as.factor) %>%
#                                              yardstick::conf_mat(truth = bleg, hardYes), 
#                                              event_level = 'second')}
# 
# 
# conf_mat[[9]]<-summary(optimised_pred[[9]]%>% 
#                                              mutate_if(is.character, as.factor) %>%
#                                              yardstick::conf_mat(truth = bleg, hardYes), 
#                                              event_level = 'second')
for(j in 1:5){{
conf_mat[[10]]<-summary(optimised_pred2[[j]][[10]]%>% 
                                             mutate_if(is.character, as.factor) %>%
                                             yardstick::conf_mat(truth = bleg, hardYes), 
                                             event_level = 'second')


conf_mat[[11]]<-summary(optimised_pred2[[j]][[11]]%>% 
                                             mutate_if(is.character, as.factor) %>%
                                             yardstick::conf_mat(truth = bleg, hardYes), 
                                             event_level = 'second')}
  conf_mat2[[j]]<-conf_mat}

rm(conf_mat)
```


```{r}
# specificity when req. sens = 0.75
spec_if_sens75<-list()
for (j in 1:5){
spec_if_sens75[[j]]<-conf_mat2[[j]][[10]] %>% filter(.metric=="spec") %>% pull(.estimate)}

#sens when req. spec = 0.97

sens_if_spec97<-list()
for (j in 1:5){
sens_if_spec97[[j]]<-conf_mat2[[j]][[11]] %>% filter(.metric=="sens") %>% pull(.estimate)}



# MaxSens_plus_Spec<-optimal[[3]] %>% select(pred_Yes) %>% pull()
reqsens_thr<-list()
for (j in 1:5){
reqsens_thr[[j]]<-optimal2[[j]][[10]] %>% select(.pred_Yes) %>% pull()}
reqspec_thr<-list()
for (j in 1:5){
reqspec_thr[[j]]<-optimal2[[j]][[11]] %>% select(.pred_Yes) %>% pull()}


```



### Predict test set (cutoff)

```{r}
#def
test_pred_def<-list()
for(j in 1:5){test_pred_def[[j]]<-catboost_testing_pred[[j]] %>%
  mutate(.pred_class=ifelse(.pred_Yes>0.5,"Yes","No"))%>%
  mutate_if(is.character,as.factor)}


test_perf_def<-list()
for(j in 1:5) {test_perf_def[[j]]<-test_pred_def[[j]] %>% 
  accuracy(truth = bleg, .pred_class) %>%
  bind_rows(test_pred_def[[j]]%>% roc_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
   bind_rows(test_pred_def[[j]]%>% pr_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
  bind_rows(test_pred_def[[j]]%>% sens(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(test_pred_def[[j]] %>% yardstick::spec(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(test_pred_def[[j]] %>% yardstick::precision(truth = bleg, .pred_class, event_level="second"))%>%
  bind_rows(test_pred_def[[j]] %>% yardstick::npv(truth = bleg, .pred_class, event_level="second"))%>%
  bind_rows(test_pred_def[[j]] %>% yardstick::f_meas(truth = bleg, .pred_class, event_level="second")) 
}

def_cm<-list()
for(j in 1:5) {def_cm[[j]]<-test_pred_def[[j]] %>% yardstick::conf_mat(truth = bleg, .pred_class)}


# reqSENS
test_pred_reqSens<-list()
for(j in 1:5){test_pred_reqSens[[j]]<-catboost_testing_pred[[j]] %>%
  mutate(.pred_class=ifelse(.pred_Yes>=reqsens_thr[[j]],"Yes","No"))%>%
  mutate_if(is.character,as.factor)}


test_perf_reqSens<-list()
for(j in 1:5) {test_perf_reqSens[[j]]<-test_pred_reqSens[[j]] %>% 
  accuracy(truth = bleg, .pred_class) %>%
  bind_rows(test_pred_reqSens[[j]]%>% roc_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
   bind_rows(test_pred_reqSens[[j]]%>% pr_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
  bind_rows(test_pred_reqSens[[j]]%>% sens(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(test_pred_reqSens[[j]] %>% yardstick::spec(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(test_pred_reqSens[[j]] %>% yardstick::precision(truth = bleg, .pred_class, event_level="second"))%>%
  bind_rows(test_pred_reqSens[[j]] %>% yardstick::npv(truth = bleg, .pred_class, event_level="second"))%>%
  bind_rows(test_pred_reqSens[[j]] %>% yardstick::f_meas(truth = bleg, .pred_class, event_level="second")) 
}

reqSens_cm<-list()
for(j in 1:5) {reqSens_cm[[j]]<-test_pred_reqSens[[j]] %>% yardstick::conf_mat(truth = bleg, .pred_class)}


# reqSpec

test_pred_reqSpec<-list()
for(j in 1:5){test_pred_reqSpec[[j]]<-catboost_testing_pred[[j]] %>%
  mutate(.pred_class=ifelse(.pred_Yes>=reqspec_thr[[j]],"Yes","No"))%>%
  mutate_if(is.character,as.factor)}


test_perf_reqSpec<-list()
for(j in 1:5) {test_perf_reqSpec[[j]]<-test_pred_reqSpec[[j]] %>% 
  accuracy(truth = bleg, .pred_class) %>%
  bind_rows(test_pred_reqSpec[[j]]%>% roc_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
   bind_rows(test_pred_reqSpec[[j]]%>% pr_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
  bind_rows(test_pred_reqSpec[[j]]%>% sens(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(test_pred_reqSpec[[j]] %>% yardstick::spec(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(test_pred_reqSpec[[j]] %>% yardstick::precision(truth = bleg, .pred_class, event_level="second"))%>%
  bind_rows(test_pred_reqSpec[[j]] %>% yardstick::npv(truth = bleg, .pred_class, event_level="second"))%>%
  bind_rows(test_pred_reqSpec[[j]] %>% yardstick::f_meas(truth = bleg, .pred_class, event_level="second")) 
}

reqSpec_cm<-list()
for(j in 1:5) {reqSpec_cm[[j]]<-test_pred_reqSpec[[j]] %>% yardstick::conf_mat(truth = bleg, .pred_class)}

```
# 
```{r}

saveRDS( bind_rows(test_perf_def, .id="Fold") %>% mutate(seed=123), "123_catboost_test_perf_def_cy.rds")
saveRDS( bind_rows(test_perf_reqSens, .id="Fold") %>% mutate(seed=123), "123_catboost_test_perf_reqSens_cy.rds")
saveRDS( bind_rows(test_perf_reqSpec, .id="Fold") %>% mutate(seed=123), "123_catboost_test_perf_reqSpec_cy.rds")
```

```{r}

test_def<-list()
for(j in 1:5) {test_def[[j]]<-test_perf_def[[j]] %>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
test_def<-do.call("rbind",test_def) %>% 
  group_by(.metric) %>% summarise_at(.vars=".estimate", funs(mean=mean, se=sd(.)/sqrt(n()), n = n()))

test_reqSens<-list()
for(j in 1:5) {test_reqSens[[j]]<-test_perf_reqSens[[j]] %>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
test_reqSens<-do.call("rbind",test_reqSens) %>% 
  group_by(.metric) %>% summarise_at(.vars=".estimate", funs(mean=mean, se=sd(.)/sqrt(n()), n = n()))



test_reqSpec<-list()
for(j in 1:5) {test_reqSpec[[j]]<-test_perf_reqSpec[[j]] %>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
test_reqSpec<-do.call("rbind",test_reqSpec) %>% 
  group_by(.metric) %>% summarise_at(.vars=".estimate", funs(mean=mean, se=sd(.)/sqrt(n()), n = n()))

```

```{r}
test_reqSens
test_reqSpec
test_def
```
```{r}
write.csv(test_reqSens, "123_catboost_test_reqSens_cy.csv")
write.csv(test_reqSpec, "123_catboost_test_reqSpec_cy.csv")
```
#by
```{r}
test_pred_reqSpec_by<-list()
for (i in 1:5){
test_pred_reqSpec_by[[i]]<-test_pred_reqSpec[[i]] %>% bind_cols(all[Outer_mlr$test.inds[[i]],]%>% select(year))}
test_pred_reqSpec_by2<-list()
for (k in 1:9){
test_pred_reqSpec_by2[[k]]<-test_pred_reqSpec_by %>% do.call(rbind,.) %>% filter(year==k+2010)}



test_perf_reqSpec_by<-list()
for(j in 1:9) {test_perf_reqSpec_by[[j]]<-test_pred_reqSpec_by2[[j]] %>% 
  accuracy(truth = bleg, .pred_class) %>%
  bind_rows(test_pred_reqSpec_by2[[j]]%>% roc_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
   bind_rows(test_pred_reqSpec_by2[[j]]%>% pr_auc(truth = bleg, .pred_Yes, event_level="second"))%>%
  bind_rows(test_pred_reqSpec_by2[[j]]%>% sens(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(test_pred_reqSpec_by2[[j]] %>% yardstick::spec(truth = bleg, .pred_class, event_level="second")) %>%
  bind_rows(test_pred_reqSpec_by2[[j]] %>% yardstick::precision(truth = bleg, .pred_class, event_level="second"))%>%
  bind_rows(test_pred_reqSpec_by2[[j]] %>% yardstick::npv(truth = bleg, .pred_class, event_level="second"))%>%
  bind_rows(test_pred_reqSpec_by2[[j]] %>% yardstick::f_meas(truth = bleg, .pred_class, event_level="second")) 
}

reqSpec_cm_by<-list()
for(j in 1:9) {reqSpec_cm_by[[j]]<-test_pred_reqSpec_by2[[j]] %>% yardstick::conf_mat(truth = bleg, .pred_class)}
```

