
# Load libraries
library(dplyr)
library(tidymodels)
library(doParallel)
library(glmnet)

# Grouped data splitting in training and testing
# It uses the field location as grouping factor
set.seed(123)
initial_split<-group_initial_split(my_db, group = field,  prop=4/5)
# Creating training and testing datasets
training_set<-training(initial_split)
testing_set<-testing(initial_split)
# Check if field locations in the training set overlap with field locations in the testing set
intersect(training_set %>% select(field) %>% pull(),
          testing_set %>% select(field) %>% pull())
# Create bootstrap indices blocking by field location
# set the number of bootstrap
n_boots<-100
boot<-group_bootstraps(training_set, group=field, times=n_boots)

# Create a recipe
lasso_rec<- 
  # exclude grouping variable "field" 
  recipe(outcome ~ ., data =training_set %>% select(-field)) %>% 
  # update role for "crop_id"
  update_role(crop_id, new_role = "cropId") %>%
  # transform categorical variables in dummy
  step_dummy(all_nominal(), - crop_id, -all_outcomes()) %>%
  # standardize variables (here is called normalize)
  step_normalize(all_predictors(), -crop_id, - all_outcomes())  %>%
  # omit NAs
  step_naomit(all_predictors())
# prepare recipe
lasso_prep <- prep(lasso_rec)

# Tune Lasso
# Set model specification (penalty = lambda, mixture = alpha). 
# LASSO logistic regression has mixture = 1
lasso_spec<-(logistic_reg(
  penalty = tune(),
  mixture = 1) %>% 
    set_mode("classification") %>%
    set_engine("glmnet"))
# Set workflow
lasso_wf<-workflow() %>%
  # Add recipe
  add_recipe(lasso_rec) %>%
  # Add model specification
  add_model(lasso_spec)
# Get HPs info
hp_arg_lasso<-tune_args(lasso_wf, verbose =T)
# Set seed to generate reproducible grid of hyperparameters (HP)
set.seed(123)
# Create HPs grid
hp_grid_lasso<-dials::parameters(lasso_wf) %>%
  update(penalty=penalty(range=c(-5,0)))%>%
  grid_latin_hypercube(size=100)
# Start Parallel
cl<-makePSOCKcluster(4)
registerDoParallel(cl)
# Set seed for reproducibility 
set.seed(123)
# Tuning results
gs_lasso_res <- tune_grid(
  # add model workflow
  lasso_wf, 
  # add resamples
  resamples = boot, 
  # add HPs grid
  grid = hp_grid_lasso,
  # set metrics
  metrics = metric_set(accuracy,
                       roc_auc,
                       pr_auc,
                       yardstick::sens,
                       yardstick::spec,
                       yardstick::precision,
                       yardstick::npv
                       ),
  # set options
  control = control_grid(
    # don't save workflow (default = F)
    save_workflow = F,
    # save predictions
    save_pred = TRUE, 
    # set verbose mode
    verbose = TRUE, 
    # set event level (according to factor level of the outcome)
    event_level = "second",
    # extract model results for each iteration
    extract = function (x) extract_model(x)))
# stop Parallel
stopCluster(cl)

#Tuning results
# Collect metrics
gs_lasso_res %>%
  collect_metrics()
# Show first 10 best scores for each metric
gs_lasso_res %>%
  show_best("accuracy")
gs_lasso_res %>%
  show_best("roc_auc")
gs_lasso_res %>%
  show_best("pr_auc")
gs_lasso_res %>%
  show_best("sens")
gs_lasso_res %>%
  show_best("spec")
gs_lasso_res %>%
  show_best("precision")
gs_lasso_res %>%
  show_best("npv")
# MaxPerf model (highest AUROC)
select_best(gs_lasso_res, "roc_auc")

# Stability function. Code from Nuogueira et al., 2017
# https://github.com/nogueirs/JMLR2018

getStability <- function(X,alpha=0.05) {
  ## the input X is a binary matrix of size M*d where:
  ## M is the number of bootstrap replicates
  ## d is the total number of features
  ## alpha is the level of significance (e.g. if alpha=0.05, we will get 95%   confidence intervals)
  ## it's an optional argument and is set to 5% by default
  ### first we compute the stability
  
  M<-nrow(X)
  d<-ncol(X)
  hatPF<-colMeans(X)
  kbar<-sum(hatPF)
  v_rand=(kbar/d)*(1-kbar/d)
  stability<-1-(M/(M-1))*mean(hatPF*(1-hatPF))/v_rand ## this is the stability estimate
  
  ## then we compute the variance of the estimate
  ki<-rowSums(X)
  phi_i<-rep(0,M)
  for(i in 1:M){ 
    phi_i[i]<-(1/v_rand)*((1/d)*sum(X[i,]*hatPF)-(ki[i]*kbar)/d^2-(stability/2)*((2*kbar*ki[i])/d^2-ki[i]/d-kbar/d+1))
  }
  phi_bar=mean(phi_i)
  var_stab=(4/M^2)*sum((phi_i-phi_bar)^2) ## this is the variance of the stability estimate
  
  ## then we calculate lower and upper limits of the confidence intervals
  z<-qnorm(1-alpha/2) # this is the standard normal cumulative inverse at a level 1-alpha/2
  upper<-stability+z*sqrt(var_stab) ## the upper bound of the (1-alpha) confidence interval
  lower<-stability-z*sqrt(var_stab) ## the lower bound of the (1-alpha) confidence interval
  
  return(list("stability"=stability,"variance"=var_stab,"lower"=lower,"upper"=upper))
  
}


# Create matrix
# set the number of variables
num_variables = dim(juice(lasso_prep))[2]

# set the number of features used in the training process (exclude crop_id and the outcome)
num_features = num_variables-2 

# set the number of lambda values
lambda<-dim(hp_grid_lasso)[1]

# create a list to collect results
lasso_fs.matrix.ls <- list()

# lasso matrix is a list. 
# Each element of the list contain a datadrame corresponding to a specific value of lambda.
# Each column of the dataframe is a variable, each row of the dataframe is a bootstrap iteration.

# create a list to store coefficients
c<-list()
for(j in 1:lambda) {
  for(i in 1:n_boots){
    # extract coefficients
    c[[i]]<-coef(
      gs_lasso_res$.extracts[[i]]$.extracts[[j]],
      s=gs_lasso_res$.metrics[[i]]$penalty[j]) %>%
      # convert the sparse matrix into a matrix
      as.matrix() %>%
      # convert into a dataframe
      as.data.frame() %>%
      
      mutate(variables = rownames(.),
             coeff = `s1`, .keep="unused") %>%
      # remove the Intercept
      filter(variables != "(Intercept)") %>%
      # convert coefficients to a binary variable: discarded (0), retained (1)
      mutate(coeff = case_when(coeff==0~0,
                               TRUE~1)) 
  }
  
  coeff<-c%>%purrr::reduce(dplyr::left_join, by = c("variables"))
  
  lasso_fs.matrix.ls[[j]]<-setNames(data.frame(t(coeff[,-1])), coeff[,1]) %>% 
    rownames_to_column() %>%  
    select(-rowname)
 }



## Compute stability 
lasso_results<-as.data.frame(array(0,dim=c(lambda,4)))

for (j in 1:lambda){
  for(i in 1:4) {
    lasso_results[j,i] <- 
      getStability( unname(as.matrix(lasso_fs.matrix.ls[[j]])),alpha=0.05)[i]
  }
}

stability<-lasso_results %>%
  mutate(stability =  V1,
         variance = V2,
         lower_bound = V3,
         upper_bound = V4, .keep="unused") %>%
  
  bind_cols(lambda = gs_lasso_res$.metrics[[1]]$penalty[c(1:lambda)]) %>%
  mutate( n = n_boots)
stability


# Average n of feature selected for each lambda
av_features<-vector()
# average number of predictors by lambda
for (j in 1:lambda){ av_features[j]<-
  lasso_fs.matrix.ls[[j]]%>% t %>% as.data.frame() %>% 
  # number of selected predictors per bootstrap
  summarise_all(sum) %>%
  rowMeans()} 
av_features<-av_features %>%
  as.data.frame()
  colnames(av_features)<-"av selected"
av_features<-av_features %>%  
  bind_cols(
  model=paste("Preprocessor1_Model", 
              sprintf('%0.3d', 1:dim(hp_grid_lasso)[1]), sep=""))


stability_vs_performance<-stability %>%
  select(lambda, stability, lower_bound, upper_bound) %>%
  left_join(gs_lasso_res %>% 
              collect_metrics() %>% 
              filter(.metric =="roc_auc") %>%
              mutate(ROC_AUC = mean,
                     lambda=penalty,.keep="unused") %>%
            select(ROC_AUC,lambda), 
            by =c("lambda")) %>%
  mutate( model = paste("Preprocessor1_Model", 
                        sprintf('%0.3d', 1:dim(hp_grid_lasso)[1]), 
                        sep=""),.keep="unused")  %>%
  na.omit()


# Identify optimal model OPT
# OPT is obtained identifying a trade-off between stability and predictive performance
acc.const<-0.01
stab.const<-0

# remove all the model with ROC_AUC <= ROC_AUC - acc.const
# print the model with best stability of the remaining models
opt<-stability_vs_performance %>% 
  filter(!ROC_AUC < max(ROC_AUC) - acc.const) %>%
  #Among the remaining configurations, determine the maximal stability stab.max.
  # Remove all configurations with stability < stab.max âˆ’ stab.const
  filter(!stability < max(stability) - stab.const) %>%
  # Among the remaining configurations, determine the maximal accuracy aUC.max.end
  # Remove all configurations with accuracy < acc.max.end.
  filter(!ROC_AUC<max(ROC_AUC)) 


# Define maxSTAB
max_stab<-stability_vs_performance%>% 
  filter(stability == max(stability)) %>% slice(1)
# Define maxPERF
max_perf<-stability_vs_performance%>% 
  filter(ROC_AUC == max(ROC_AUC)) %>% slice(1)
opt
max_stab
max_perf


final_models<-
  max_perf %>% 
  select(penalty=lambda,.config=model)%>%
  mutate(method="maxPerf")%>%
  bind_rows(opt %>% 
              select(penalty=lambda,.config=model)%>% 
              mutate(method="Opt"))%>%
  bind_rows(max_stab %>% select(penalty=lambda,.config=model)%>%
              mutate(method="maxstab"))

final_models

# finalize workflow
final_lasso<-list()
for(i in 1:3){
  final_lasso[[i]]<-
    lasso_wf %>% 
    finalize_workflow(final_models[i,])}

# start Parallell
library(doParallel)
cl<-makePSOCKcluster(4)
registerDoParallel(cl)
# set seed for reproducibility
set.seed(123)
# Fit all the training data
final_fit<-list()
for(i in 1:3){final_fit[[i]] <- final_lasso[[i]] %>%
  fit(training_set %>% select(-field))}


# extract coefficients  
coeff_lasso<-list()
for(i in  1:3){coeff_lasso[[i]] <-final_fit[[i]]%>%
  pull_workflow_fit() %>%
  tidy() }


# stop parallel
stopCluster(cl)


# Test models on the indipendent test set

test_pred<-list()

for (i in 1:3){
  test_pred[[i]] <- 
  predict(final_fit[[i]], testing_set) %>% 
  bind_cols(predict(final_fit[[i]], testing_set, type = "prob")) %>% 
  bind_cols(testing_set %>% select(outcome)) %>%
  bind_cols(method=final_models$method[[i]])%>%
  mutate_if(is.character, as.factor)}

test_perf<-list()

for (i in 1:3){
  test_perf[[i]] <- 
  test_pred[[i]] %>% 
  accuracy(truth = outcome, .pred_class) %>%
  bind_rows(test_pred[[i]]%>% roc_auc(truth = outcome, .pred_Yes, event_level="second"))%>%
  bind_rows(test_pred[[i]]%>% pr_auc(truth = outcome, .pred_Yes, event_level="second"))%>%
  bind_rows(test_pred[[i]]%>% sens(truth = outcome, .pred_class, event_level="second")) %>%
  bind_rows(test_pred[[i]] %>% yardstick::spec(truth = outcome, .pred_class, event_level="second")) %>%
  bind_rows(test_pred[[i]] %>% yardstick::precision(truth = outcome, .pred_class, event_level="second")) %>%
  bind_rows(test_pred[[i]] %>% yardstick::npv(truth = outcome, .pred_class, event_level="second")) %>%
  bind_cols(method=final_models$method[[i]])}



