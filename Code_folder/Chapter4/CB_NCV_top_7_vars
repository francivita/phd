#title: "Recursive Feature Elimination with CatBoost 2"
#author: "Francesco Civita"
#date: "16/02/2023"

# This is code is to train a model with top 7 features derived from the 
# CB_NCV_first_run

library(dplyr)
library(tidymodels)
library(skimr)
library(tictoc)
library(remotes) # to install treesnip

# install treesnip install_github("curso-r/treesnip")
# install_github("Glemhel/treesnip")
# install catboost devtools::install_url('https://github.com/catboost/catboost/releases/download/v1.1.1/catboost-R-Windows-1.1.1.tgz', INSTALL_opts = c("--no-multiarch", "--no-test-load"))
library(catboost)
library(treesnip)
set_dependency("boost_tree", eng = "catboost", "catboost")
set_dependency("boost_tree", eng = "catboost", "treesnip")

my_db<-readRDS("my_db.rds")
cross_year_ncv<-readRDS("cross_year_ncv.rds")
merge_fi<-readRDS("merge_fi.rds")
hp_grid_catboost<-readRDS("hp_grid_catboost.rds")

fi_7<-list()

  for(i in 1:5){
    fi_7[[i]]<-merge_fi %>% filter(Fold==i) %>% slice(1:7) %>% pull(rowname)}




# Recipe for preprocessing. 
# Remove the variable "year" and assign ID role to crop_id

recipe_7<-list()

for(j in 1:5){
  recipe_7[[j]]<- recipe(outcome ~ ., data = my_db[Outer_mlr$train.inds[[j]],] %>% 
                           select(crop_id, fi_7[[j]], outcome)) %>%
    update_role(crop_id, new_role = "Id") 
  
}

skim(juice(prep(recipe_7[[1]])))


library(catboost)
library(treesnip)
set_dependency("boost_tree", eng = "catboost", "catboost")
set_dependency("boost_tree", eng = "catboost", "treesnip")

catboost_spec_7<-(boost_tree(
  mtry = tune(),
  trees = 150,
  tree_depth = tune(),
  learn_rate = tune()) %>% 
    set_mode("classification") %>%
    set_engine("catboost", nthread=4, loss_function="Logloss", 
               eval_metric="AUC"))


catboost_wf_7<-list()
for(j in 1:5){
  catboost_wf_7[[j]]<-workflow() %>%
    add_recipe(recipe_7[[j]]) %>%
    add_model(catboost_spec_7)}




library(tictoc)
library(doParallel)
cl<-makePSOCKcluster(4)
registerDoParallel(cl)
tic()

gs_catboost_res_7<-list()
#try different seeds
set.seed(123, kind = "L'Ecuyer-CMRG")

for(j in 1:5){
  gs_catboost_res_7[[j]] <- tune_grid(
    catboost_wf_7[[j]], # Model workflow defined above
    resamples = cross_year_ncv[[j]],          # Resamples defined obove
    grid = hp_grid_catboost,            # Number of candidate parameter sets to be created    automatically
    metrics = metric_set(accuracy,
                         pr_auc,
                         roc_auc,
                         sens,
                         yardstick::spec,
                         yardstick::precision,
                         yardstick::npv
    ),    # metric
    control = control_grid(save_workflow = T, save_pred = TRUE, verbose = TRUE, event_level = "second",
                           extract = function (x) extract_fit_engine(x)))}# control the tuning process



toc()
stopCluster(cl)

unregister <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}
unregister()

### Collect metrics catboost
metric_catboost_7<-list()
for (j in 1:5) {metric_catboost_7[[j]]<- gs_catboost_res_7[[j]] %>%
  collect_metrics()}

saveRDS(metric_catboost_7, "metric_catboost_cy_7.rds")


### Best AUROC catboost

best_auc_gs_catboost_7<-list()
for(j in 1:5){
  best_auc_gs_catboost_7[[j]]<-select_best(gs_catboost_res_7[[j]], "roc_auc")}



### Collect metrics catboost Sf

metric_catboost_all_7<-list()
for (j in 1:5) {metric_catboost_all_7[[j]]<- gs_catboost_res_7[[j]] %>%
  collect_metrics(summarize=F)}

metric_catboost_Sf_7<-list()
for (j in 1:5) {metric_catboost_Sf_7[[j]]<- metric_catboost_all_7[[j]]%>% 
  filter(.config==best_auc_gs_catboost_7[[j]]$.config)%>%
  mutate(model="catboost")}

saveRDS(metric_catboost_all_7, "metric_catboost_all_7.rds")
saveRDS(metric_catboost_Sf_7, "metric_catboost_Sf_7.rds")


### Collect predictions catboost

predictions_all_7<-list()
for(i in 1:5) {predictions_all_7[[i]]<-collect_predictions(gs_catboost_res_7[[i]], summarize=F)}

predictions_7<-list()
for(i in 1:5) {predictions_7[[i]]<-predictions_all_7[[i]]%>% filter(.config == best_auc_gs_catboost_7[[i]]$.config) %>%
  select(id,.row, outcome, catboost_Yes=.pred_Yes, catboost_No = .pred_No, catboost_pred=.pred_class)}

saveRDS(predictions_7, "predictions_catboost_7.rds")
saveRDS(predictions_all_7, "predictions_all_catboost_7.rds")


### Finalize catboost (best AUROC)


final_catboost_7<-list()
for(j in 1:5){
  final_catboost_7[[j]]<-catboost_wf_7[[j]] %>% 
    finalize_workflow(best_auc_gs_catboost_7[[j]])}


#### Last fit catboost (best AUROC)



cl<-makePSOCKcluster(4)
registerDoParallel(cl)
tic()

#try different seeds
set.seed(123, kind = "L'Ecuyer-CMRG")
catboost_fit_7<-list()
for(j in 1:5){
  catboost_fit_7[[j]]<-final_catboost_7[[j]] %>%
    fit(my_db[Outer_mlr$train.inds[[j]],])}

toc()
stopCluster(cl)
unregister()


# Feature importance

overall_fi_7<-list()
for(j in 1:5){
  overall_fi_7[[j]]<-extract_fit_engine(catboost_fit_7[[j]])}


merge_fi_7<-list()
for (j in 1:5) {merge_fi_7[[j]]<-overall_fi_7[[j]]$feature_importances %>% 
  as.data.frame() %>%rownames_to_column()%>% 
  arrange(desc(V1)) %>% mutate(Fold = j)}


saveRDS(merge_fi_7, "merge_fi_7.rds")


### Perf assessment



# train set predictions
catboost_training_pred_7<-list()
for(j in 1:5){catboost_training_pred_7[[j]] <- 
  predict(catboost_fit_7[[j]], my_db[Outer_mlr$train.inds[[j]],]) %>% 
  bind_cols(predict(catboost_fit_7[[j]],my_db[Outer_mlr$train.inds[[j]],], type = "prob")) %>% 
  bind_cols(my_db[Outer_mlr$train.inds[[j]],] %>% select(outcome))}

catboost_training_perf_7<-list()
for(j in 1:5) {catboost_training_perf_7[[j]]<-catboost_training_pred_7[[j]] %>% 
  accuracy(truth = outcome, .pred_class) %>%
  bind_rows(catboost_training_pred_7[[j]]%>% roc_auc(truth = outcome, .pred_Yes, event_level="second"))%>%
  bind_rows(catboost_training_pred_7[[j]]%>% pr_auc(truth = outcome, .pred_Yes, event_level="second"))%>%
  bind_rows(catboost_training_pred_7[[j]]%>% sens(truth = outcome, .pred_class, event_level="second")) %>%
  bind_rows(catboost_training_pred_7[[j]] %>% yardstick::spec(truth = outcome, .pred_class, event_level="second")) %>%
  bind_rows(catboost_training_pred_7[[j]] %>% yardstick::precision(truth = outcome, .pred_class, event_level="second")) %>%
  bind_rows(catboost_training_pred_7[[j]] %>% yardstick::npv(truth = outcome, .pred_class, event_level="second"))}

catboost_train_cm_7<-list()
for(j in 1:5) {catboost_train_cm_7[[j]]<-catboost_training_pred_7[[j]] %>% yardstick::conf_mat(truth = outcome, .pred_class)}




# train set predictions
catboost_testing_pred_7<-list()
for(j in 1:5){catboost_testing_pred_7[[j]] <- 
  predict(catboost_fit_7[[j]], my_db[Outer_mlr$test.inds[[j]],]) %>% 
  bind_cols(predict(catboost_fit_7[[j]],my_db[Outer_mlr$test.inds[[j]],], type = "prob")) %>% 
  bind_cols(my_db[Outer_mlr$test.inds[[j]],] %>% select(outcome))}

catboost_testing_perf_7<-list()
for(j in 1:5) {catboost_testing_perf_7[[j]]<-catboost_testing_pred_7[[j]] %>% 
  accuracy(truth = outcome, .pred_class) %>%
  bind_rows(catboost_testing_pred_7[[j]]%>% pr_auc(truth = outcome, .pred_Yes, event_level="second"))%>%
  bind_rows(catboost_testing_pred_7[[j]]%>% roc_auc(truth = outcome, .pred_Yes, event_level="second"))%>%
  bind_rows(catboost_testing_pred_7[[j]]%>% sens(truth = outcome, .pred_class, event_level="second")) %>%
  bind_rows(catboost_testing_pred_7[[j]] %>% yardstick::spec(truth = outcome, .pred_class, event_level="second")) %>%
  bind_rows(catboost_testing_pred_7[[j]] %>% yardstick::precision(truth = outcome, .pred_class, event_level="second"))%>%
  bind_rows(catboost_testing_pred_7[[j]] %>% yardstick::npv(truth = outcome, .pred_class, event_level="second")) 
}

catboost_test_cm_7<-list()
for(j in 1:5) {catboost_test_cm_7[[j]]<-catboost_testing_pred_7[[j]] %>% yardstick::conf_mat(truth = outcome, .pred_class)}


saveRDS( bind_rows(catboost_testing_perf_7, .id="Fold") %>% mutate(seed=123), "catboost_testing_perf_7.rds")
saveRDS( bind_rows(catboost_training_perf_7, .id="Fold") %>% mutate(seed=123), "catboost_training_perf_7.rds")

# save this for cutoff analysis
saveRDS(catboost_testing_pred_7,"catboost_testing_pred_7.rds")

# aggregate validation metrics (summarize = T)
validation_7<-list()
for(j in 1:5) {validation_7 [[j]]<-metric_catboost_7[[j]] %>% filter(.config == best_auc_gs_catboost_7[[j]]$.config)%>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
validation_7<-do.call("rbind", validation_7)


# aggregate validation metrics (summarize = F)

validation_sF_7<-list()
for(j in 1:5) {validation_sF_7[[j]]<-collect_metrics(gs_catboost_res_7[[j]],summarize=F) %>% filter(.config == best_auc_gs_catboost_7[[j]]$.config)%>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
validation_sF_7<-do.call("rbind", validation_sF_7) %>% 
  group_by(.metric) %>% summarise_at(.vars=".estimate", funs(mean=mean, se=sd(.)/sqrt(n()), n = n()))


# aggregate train metrics

train_7<-list()
for(j in 1:5) {train_7[[j]]<-catboost_training_perf_7[[j]] %>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
train_7<-do.call("rbind", train_7) %>% 
  group_by(.metric) %>% summarise_at(.vars=".estimate", funs(mean=mean, se=sd(.)/sqrt(n()), n = n()))



# aggregate test metrics

test_7<-list()
for(j in 1:5) {test_7[[j]]<-catboost_testing_perf_7[[j]] %>% 
  bind_cols(Fold = c(paste("Fold", sprintf('%0.1d', j), sep=""))) }
test_7<-do.call("rbind", test_7) %>% 
  group_by(.metric) %>% summarise_at(.vars=".estimate", funs(mean=mean, se=sd(.)/sqrt(n()), n = n()))


# compare models

train_7
validation_sF_7
test_7
